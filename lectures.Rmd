---
title: "Bayesian statistics"
author: "Olivier Gimenez"
date: "July 2020"
output:
  beamer_presentation:
    latex_engine: xelatex
  slidy_presentation: default
  ioslides_presentation: default
header-includes: \usetheme[titleformat=smallcaps, progressbar=frametitle]{metropolis}
classoption: aspectratio=169
---

# Bayesian inference 

## Introduction
* The Bayesian approach dates back to 18th century to Reverend Thomas Bayes.
```{r, out.width = '10cm',out.height='3cm',fig.align='center',echo=FALSE}
knitr::include_graphics('img/Thomas_Bayes.png')    
```
* However, due to practical problems of implementing the Bayesian approach, little advance was made for over two centuries.
* Recent advances in computational power coupled with the development of new methodology have led to a great increase in the application of Bayesian methods within the last two decades.




## Classical versus Bayesian	
* Typical stats problems involve estimating parameter $\theta$ with available data.
* The frequentist approach (maximum likelihood estimation – MLE) assumes that the parameters are fixed, but have unknown values to be estimated.
* Classical estimates generally provide a point estimate of the parameter of interest.
* The Bayesian approach assumes that the parameters are not fixed but have some fixed  unknown distribution - a distribution for the parameter.
```{r, out.width = '10cm',out.height='3.5cm',fig.align='center',echo=FALSE}
knitr::include_graphics('img/bayesian_evol.png')    
```

## What is the Bayesian Approach?	
* The approach is based upon the idea that the experimenter begins with some prior beliefs about the system.
* And then updates these beliefs on the basis of observed data.
* This updating procedure is based upon what is known as Bayes’ Theorem:

$$\Pr(A \mid B) = \frac{\Pr(B \mid A) \; \Pr(A)}{\Pr(B)}$$

## What is the Bayesian Approach?	

* Schematically, if $A = \theta$ and $B = \text{data}$
* The Bayes’ Theorem

$$\Pr(A \mid B) = \frac{\Pr(B \mid A) \; \Pr(A)}{\Pr(B)}$$

* Translates into:

$$\Pr(\theta \mid \text{data}) = \frac{\Pr(\text{data} \mid \theta) \; \Pr(\theta)}{\Pr(\text{data})}$$

## Bayes formula	

$${\color{red}{\Pr(\theta \mid \text{data})}} = \frac{\color{blue}{\Pr(\text{data} \mid \theta)} \; \color{green}{\Pr(\theta)}}{\color{orange}{\Pr(\text{data})}}$$

* \textcolor{red}{Posterior distribution}: the basis for inference, a distribution, possibly multivariate if more than one parameter ($\theta$)

* \textcolor{blue}{Likelihood}: we know that guy from before, same as in the MLE approach

* \textcolor{green}{Prior distribution}: the source of much discussion about the Bayesian approach

* $\color{orange}{\Pr(\text{data}) = \int L(\text{data} \mid \theta) \;\Pr(\theta) d\theta }$: possibly high-dimensional integral, difficult if not impossible to calculate. This is one of the reasons why we need simulation (MCMC) methods - more soon.

## A Simple Example
* Let us take a simple example to fix ideas
* 120 deer were radio-tracked over winter
* 61 close to a plant, 59 far from any human activity
* Question: is there a treatment effect on survival?

|            | Released   | Alive | Dead | Other |
|------------+----------+-------+------+-------| 
| treatment  | 61 | 19 | 38 | 4 |
| control    | 59 | 21 | 38 | 0 |


## A Simple Example
* So, $n = 57$ deer were assigned to the treatment group of which $k=19$ survived the winter.
* Of interest is the probability of over-winter survival, call it $\theta$, for the general population within the treatment area.
* The obvious estimate is simply to take the ratio $k/n=19/57$.
* How would the classical statistician justify this estimate?

## A Simple Example
* Our model is that we have a Binomial experiment (assuming independent and identically distributed draws from the population) 
* $K$ the number of alive individuals at the end of the winter, so that $P(K=k) = \binom{n}{k}\theta^k(1-\theta)^{n-k}$

* The classical approach is to maximise the corresponding likelihood with respect to $\theta$ to obtain the entirely plausible MLE:

$$ \hat{\theta} = k/n = 19/57$$.

* Remember lecture on likelihoods

## The Bayesian Approach
* The Bayesian starts off with a prior.
* Now, the one thing we know about $\theta$ is that is a continuous random variable and that it lies between zero and one.
* Thus, a suitable prior distribution might be the Beta which is defined on this range $[0,1]$.
* What is the Beta distribution?

## What is the Beta distribution?

```{r, echo=FALSE} 
x <- seq(0, 1, length=200)
# distribution a posteriori beta
plot(x,dbeta(x, 1, 1),type='l',xlab='q',ylab='',main='',lwd=3,col='red',ylim=c(0,2.5))
points(x,dbeta(x, 2, 2),type='l',lwd=3,col='green')
points(x,dbeta(x, 5, 5),type='l',lwd=3,col='blue')
legend('topright',legend=c('beta(1,1)','beta(2,2)','beta(5,5)'),lty=c(1,1,1),col=c('red','green','blue'))
```

## The Bayesian Approach

* Suppose we assume a priori that $\theta \sim Beta(a,b)$ so that $\Pr(\theta) = \theta^{a-1} (1 - \theta)^{b-1}$

* Then we have:

$$
\begin{aligned}
{\color{red}{Pr(\theta \mid k)}} & \propto {\color{blue}{\binom{n}{k}\theta^k(1-\theta)^{n-k}}} \; {\color{green}{\theta^{a-1} (1 - \theta)^{b-1}}}\\
& \propto {\theta^{(a+k)-1}} {(1-\theta)^{(b+n-k)-1}} 
\end{aligned}
$$

* That is: 

$$ \theta \mid k \sim Beta(a+k,b+n-k)$$

* Take a Beta prior with a Binomial likelihood, you get a Beta posterior (conjugacy)

## Application to the deer example

* We have that survival $\theta\mid k \sim Beta(a+k,b+n-k)$

* The posterior has an explicit expression, easy to manipulate

* $E(\theta \mid k) = \displaystyle{\frac{a + k}{n+a+b}}$ 

* $V(\theta \mid k) = \displaystyle{\frac{(a+k)(b+n-k)}{(n+a+b)^2(n+a+b+1)}}$

* If we take a Uniform prior, i.e. $Beta(1,1)$, then we have 

* $\theta_{treatment} \sim Beta(1+19,1+57-19)=Beta(20,39)$

* $E(\theta_{treatment}) = 0.339$ and $V(\theta_{treatment}) = 0.061$

## Prior $Beta(1,1)$ and posterior survival $Beta(20,39)$
```{r echo=FALSE}
x <- seq(0, 1, length=200)
# distribution a posteriori beta
plot(x,dbeta(x, 20,39),type='l',xlab='',ylab='',main='',lwd=3,col='red')
# distribution a priori uniforme
points(x,dbeta(x, 1, 1),type='l',lwd=3)
```

## Prior $Beta(0.5,0.5)$ and posterior survival $Beta(19.5,38.5)$
```{r echo=FALSE}
x <- seq(0, 1, length=200)
# distribution a posteriori beta
plot(x,dbeta(x, .5+19,.5+57-19),type='l',xlab='',ylab='',main='',lwd=3,col='red')
# distribution a priori uniforme
points(x,dbeta(x, .5, .5),type='l',lwd=3)
```

## Prior $Beta(2,2)$ and posterior survival $Beta(21,40)$
```{r echo=FALSE}
x <- seq(0, 1, length=200)
# distribution a posteriori beta
plot(x,dbeta(x, 2+19,2+57-19),type='l',xlab='',ylab='',main='',lwd=3,col='red')
# distribution a priori uniforme
points(x,dbeta(x, 2, 2),type='l',lwd=3)
```

## Prior $Beta(20,1)$ and posterior survival $Beta(39,49)$
```{r echo=FALSE}
x <- seq(0, 1, length=200)
# distribution a posteriori beta
plot(x,dbeta(x, 20+19,1+57-19),type='l',xlab='',ylab='',main='',lwd=3,col='red')
# distribution a priori uniforme
points(x,dbeta(x, 20, 1),type='l',lwd=3)
```

## The Role of the Prior
* In biological applications, the prior is a convenient means of incorporating expert opinion or information from previous or related studies that would otherwise need to be ignored.
* With sparse data, the role of the prior can be to enable inference on key parameters that would otherwise be impossible.
* With sufficiently large and informative datasets the prior typically has little effect on the results.
* Always perform a sensitivity analysis!

## Informative Priors / No Information
* Informative priors aim to reflect information available to the analyst that is gained independently of the data being studied.
* In the absence of any prior information on one or more model parameters we wish to ensure that this lack of knowledge is properly reflected in the prior.
* Always perform a sensitivity analysis!

## Back to the Bayes formula	

* Bayes inference is easy! Well, not so easy in real-life applications...

* The issue is in ${\Pr(\theta \mid \text{data})} = \displaystyle{\frac{{\Pr(\text{data} \mid \theta)} \; {\Pr(\theta)}}{\color{orange}{\Pr(\text{data})}}}$

* $\color{orange}{\Pr(\text{data}) = \int{L(\text{data} \mid \theta)\Pr(\theta) d\theta}}$ is a $N$-dimensional integral if $\theta = \theta_1, \ldots, \theta_N$ 

* Difficult, if not impossible to calculate! 

* Until recently, Bayesian analysis of complex models not possible

## Bayesian computation

* In the early 1990s, statisticians rediscovered work from the 1950's in physics

```{r, out.width = '9cm',out.height='3cm',fig.align='center',echo=FALSE}
knitr::include_graphics('img/metropolis.png')   
```

* Use stochastic simulation to draw samples from posterior distributions

* Avoid explicit calculation of integrals in Bayes formula

* Instead, approximate posterior to arbitrary degree of precision by drawing large sample

* Markov chain Monte Carlo = MCMC; boost to Bayesian statistics!

## MANIAC

```{r, out.width = '11cm',out.height='7cm',fig.align='center',echo=FALSE}
knitr::include_graphics('img/maniac.png')   
```

## Why are MCMC methods so useful?

* MCMC: stochastic algorithm to produce sequence of dependent random numbers (from Markov chain)

* Converge to equilibrium (aka stationary) distribution

* Equilibrium distribution IS the desired posterior distribution

* Several ways of constructing these chains: Metropolis-Hastings, Gibbs sampler, Metropolis-within-Gibbs, ...

* How to implement them in practice?!

## In practice, when is equilibrium attained?

* Run multiple chains from arbitrary starting places (inits)

* Assume convergence when all chains reach same regime

* Discard initial burn-in phase

* Summarize posterior distribution with mean, sd and credible intervals

```{r, out.width = '11cm',out.height='7cm',fig.align='center',echo=FALSE}
knitr::include_graphics('img/mcmc.png')   
```

## Introduction to JAGS (Just Another Gibbs Sampler)

\begin{center}
Martyn Plummer
\end{center}
```{r, out.width = '9cm',out.height='5cm',fig.align='center',echo=FALSE}
knitr::include_graphics('img/plummer.png') 
```

## Let's redo the logistic regression with the White stork data

* We'll need data

* We'll need to build a model - write down the likelihood 

* We'll need to specify priors for parameters

## Let us read in the data
```{r}
nbsuccess = c(151,105,73,107,113,87,77,108,118,122,112,120,122,89,69,71,
              53,41,53,31,35,14,18)
nbpairs = c(173,164,103,113,122,112,98,121,132,136,133,137,145,117,90,80,
            67,54,58,39,42,23,23)
temp = c(15.1,13.3,15.3,13.3,14.6,15.6,13.1,13.1,15.0,11.7,15.3,14.4,14.4,
         12.7,11.7,11.9,15.9,13.4,14.0,13.9,12.9,15.1,13.0)
rain = c(67,52,88,61,32,36,72,43,92,32,86,28,57,55,66,26,28,96,48,90,86,
           78,87)
datax = list(N=23,nbsuccess = nbsuccess,nbpairs = nbpairs, 
             temp = temp,rain = rain)
```

## What is the model?

$$
\begin{aligned}
\text{nbchicks}_i \sim \text{Binomial(nbpairs}_i,p_i) \\
\text{logit}(p_i) = a + b_{temp} \; \text{temp}_{i} + b_{rain} \; \text{rainfall}_{i}\\
\end{aligned}
$$

## Let us build the model

```{r, echo=TRUE, eval=FALSE}
{
# Likelihood
  	for( i in 1 : N){
		nbsuccess[i] ~ dbin(p[i],nbpairs[i])
		logit(p[i]) <- a + b.temp * temp[i] + b.rain * rain[i]
		}
# ...
```

## Let us specify priors 
```{r, echo=TRUE, eval=FALSE}
{
# Likelihood
  	for( i in 1 : N){
		nbsuccess[i] ~ dbin(p[i],nbpairs[i])
		logit(p[i]) <- a + b.temp * temp[i] + b.rain * rain[i]
		}
# Priors
a ~ dnorm(0,0.001)
b.temp ~ dnorm(0,0.01)
b.rain ~ dnorm(0,0.01)
}
```
**Warning**: Jags uses precision for Normal distributions (1 / variance)

```{r message=FALSE, warning=FALSE, include=FALSE}
model <- 
paste("
model
{
	for( i in 1 : N) 
		{
		nbsuccess[i] ~ dbin(p[i],nbpairs[i])
		logit(p[i]) <- a + b.temp * temp[i] + b.rain * rain[i]
		}
			
# priors for regression parameters
a ~ dnorm(0,0.001)
b.temp ~ dnorm(0,0.001)
b.rain ~ dnorm(0,0.001)
			
	}
")
writeLines(model,"code/reglogistique.txt")
```

## Let us specify a few additional things

```{r message=FALSE, warning=FALSE}
# list of lists of initial values (one for each MCMC chain)
init1 = list(a=-.5)
init2 = list(a=.5)
inits = list(init1,init2)

# specify parameters that need to be estimated
parameters <- c("a","b.temp","b.rain")

# specify nb iterations for burn-in and final inference 
nb.burnin = 1000
nb.iterations = 2500
```

## Let us run Jags!

```{r message=FALSE, warning=FALSE}
# load R2jags to run Jags through R
library(R2jags)
reglogcig.sample <- jags(datax,inits,parameters,n.iter=nb.iterations,
                         model.file="code/reglogistique.txt",
                         n.chains=2,n.burnin=nb.burnin)
```

## Display parameter estimates
```{r message=FALSE, warning=FALSE}
reglogcig.sample
```

## Let us assess convergence

```{r message=FALSE, warning=FALSE}
traceplot(reglogcig.sample,mfrow = c(3, 1),
          varname=c('a','b.rain','b.temp'))
```

## Let us explore the results
```{r message=FALSE, warning=FALSE}
res = as.mcmc(reglogcig.sample) # convert outputs in a list
res = rbind(res[[1]],res[[2]]) # put two MCMC lists on top of each other
head(res)
```

## Compute a posteriori Pr(rain < 0)
```{r message=FALSE, warning=FALSE}
# probability that the effect of rainfall is negative
mean(res[,'b.rain'] < 0)
```

## Compute a posteriori Pr(temp < 0)
```{r message=FALSE, warning=FALSE}
# probability that the effect of temperature is negative
mean(res[,'b.temp'] < 0)
```

## Get credible interval for the rain effect
```{r message=FALSE, warning=FALSE}
quantile(res[,'b.rain'],c(0.025,0.975))
```

## Get credible interval for the temperature effect
```{r message=FALSE, warning=FALSE}
quantile(res[,'b.temp'],c(0.025,0.975))
```

## Graphical summaries
```{r echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
plot(density(res[,'b.rain']),xlab="",ylab="", main="a posteriori density 
     of the rainfall effect ",lwd=3)
abline(v=0,col='red',lwd=2)
plot(density(res[,'b.temp']),xlab="",ylab="", main="a posteriori density 
     of the temperature effect ",lwd=3)
abline(v=0,col='red',lwd=2)
```

There is an influence of rainfall, but not temperature (credible interval does not and does contain 0)

```{r echo=FALSE, message=FALSE, warning=FALSE}
# read in data
data <- as.matrix(read.table("dat/dipper.dat"))

# number of individuals 
n <- dim(data)[[1]] 

# number of capture occasions
K <- dim(data)[[2]] 

# compute the date of first capture
e <- NULL
for (i in 1:n){
	temp <- 1:K
	e <- c(e,min(temp[data[i,]==1]))
	}

# data
datax <- list(N=n,Years=K,obs=data,First=e)

# mark-recapture analysis for European Dippers
model <- 
paste("
model
{
for (i in 1:N){
	alive[i,First[i]] <- 1
	for (j in (First[i]+1):Years){
		alive[i,j] ~ dbern(alivep[i,j])
		alivep[i,j] <- surv * alive[i,j-1]
		obs[i,j] ~ dbern(sightp[i,j])
		sightp[i,j] <- resight * alive[i,j]
		}
	}
surv~dunif(0,1)
resight~dunif(0,1)
}
")
writeLines(model,"code/CJS.txt")

# In JAGS we have to give good initial values for the latent state alive. At all occasions when an individual was observed, its state is alive = 1 for sure. In addition, if an individual was not observed at an occasion, but was alive for sure, because it was observed before and thereafter (i.e. has a capture history of e.g. {101} or {10001}), then we know that the individual was alive at all of these occasions, and thus alive = 1. Therefore, we should provide initial values of alive = 1 at these positions as well. The following function provides such initial values from the observed capture histories (from Kery and Schaub book)

known.state.cjs <- function(ch){
   state <- ch
   for (i in 1:dim(ch)[1]){
      n1 <- min(which(ch[i,]==1))
      n2 <- max(which(ch[i,]==1))
      state[i,n1:n2] <- 1
      state[i,n1] <- NA
      }
   state[state==0] <- NA
   return(state)
   }

Xinit <- known.state.cjs(data)

# first list of inits
init1 <- list(surv=.1,resight=.1,alive=Xinit)
# second list of inits
init2 <- list(surv=.9,resight=.9,alive=Xinit)

# specify the parameters to be monitored
parameters <- c("resight","surv")

# load R2jags
library(R2jags)

# run the MCMC analysis WITHOUT PRIOR INFORMATION
CJS.sim <-jags(data=datax, inits=list(init1,init2), parameters,n.iter=1000,model.file="code/CJS.txt",n.chains=2,n.burnin=500)

# to see the numerical results
# CJS.sim
# traceplot(CJS.sim) # diagnostic de convergence

# keep 3 first years only
data = data[,1:3]
databis = NULL
for (i in 1:nrow(data)){
	# discard all non existing individuals i.e. those that were never captured
	# test whether there was at least 1 detection and keep this individual if it was the case
	if (sum(data[i,] == c(0,0,0))<3)  databis = rbind(databis,data[i,])
	}
data = databis

# number of individuals 
n <- dim(data)[[1]] 

# number of capture occasions
K <- dim(data)[[2]] 

# compute the date of first capture
e <- NULL
for (i in 1:n){
	temp <- 1:K
	e <- c(e,min(temp[data[i,]==1]))
	}

# data
datax <- list(N=n,Years=K,obs=data,First=e)

Xinit <- known.state.cjs(data)

# first list of inits
init1 <- list(surv=.1,resight=.1,alive=Xinit)
# second list of inits
init2 <- list(surv=.9,resight=.9,alive=Xinit)

# specify the parameters to be monitored
parameters <- c("resight","surv")

# run the MCMC analysis WITHOUT PRIOR INFORMATION
CJS.sim.wo.apriori <-jags(data=datax, inits=list(init1,init2), parameters,n.iter=1000,model.file="code/CJS.txt",n.chains=2,n.burnin=500)

# same model but with informative prior on survival 
model <- 
paste("
model
{
for (i in 1:N){
	alive[i,First[i]] <- 1
	for (j in (First[i]+1):Years){
		alive[i,j] ~ dbern(alivep[i,j])
		alivep[i,j] <- surv * alive[i,j-1]
		obs[i,j] ~ dbern(sightp[i,j])
		sightp[i,j] <- resight * alive[i,j]
		}
	}
surv~dnorm(0.57,187.6) # Norm(0.57,sd=0.073) ; precision = 1/var = 1/0.073^2
resight~dunif(0,1)
}
")
writeLines(model,"code/CJS2.txt")

CJS.sim.apriori <-jags(data=datax, inits=list(init1,init2), parameters,n.iter=1000,model.file="code/CJS2.txt",n.chains=2,n.burnin=500)
```

## How to incorporate prior information? A capture-recapture example

* Estimating survival using capture-recapture data

* E.g. 101 i.e. captured, missed and recaptured

* Simplest model: constant survival $\phi$ and detection $p$ probabilities 

$$\Pr(101) = \phi (1-p) \phi p $$

* Assuming a vague prior 

$$\phi_{prior} \sim \text{Uniform}(0,1)$$

## Case study

* European dippers in Eastern France (1981-1987)
```{r, out.width = '10cm',out.height='3cm',fig.align='center',echo=FALSE}
knitr::include_graphics('img/dipper.png')    
```

* Mean posterior is $\phi_{posterior} = 0.56$ ith credible interval $[0.51,0.61]$

## How to incorporate prior information?

* Using information on body mass and annual survival of 27 European passerines, we can predict survival of European dippers using only body mass

* For dippers, body mass is 59.8g, therefore $\phi = 0.57$ with $\text{sd} = 0.073$

* Assuming an informative prior $\phi_{prior} \sim \text{Normal}(0.57,0.073)$

* Mean posterior $\phi_{posterior} = 0.56$ with credible interval $[0.52, 0.60]$

* No increase of precision in posterior inference 

## A general result

\textcolor{red}{This is a general result, the Bayesian and frequentist estimates will always agree if there is sufficient data, so long as the likelihood is not explicitly ruled out by the prior}

## How to incorporate prior information?

* Using information on body mass and annual survival of 27 European passerines, we can predict survival of European dippers using only body mass

* For dippers, body mass is 59.8g, therefore $\phi = 0.57$ with $\text{sd} = 0.073$

* Assuming an informative prior $\phi_{prior} \sim \text{Normal}(0.57,0.073)$ 

* **With 3 first years only**

* Width of credible interval is 0.47 (vague prior) vs. 0.30 (informative prior) 

* Huge increase of precision in posterior inference ($40\%$ gain)!

## Compare \textcolor{blue}{vague} vs. \textcolor{red}{informative} prior

```{r echo=FALSE, message=FALSE, warning=FALSE}
res = as.mcmc(CJS.sim.wo.apriori) 
res = rbind(res[[1]],res[[2]]) 
#head(res)

res2 = as.mcmc(CJS.sim.apriori) 
res2 = rbind(res2[[1]],res2[[2]]) 
#head(res2)

plot(density(res2[,'surv']),xlab='survival',ylab='probability density',col='red',lwd=4,main='',xlim=c(0.2,1))
lines(density(res[,'surv']),xlab='survival',ylab='probability density',col='blue',lwd=4,main='')
legend('topleft',lwd=2,legend=c('with prior info','without prior info'),col=c('red','blue'))
```

# Multilevel models

## Model with **complete data pooling**

Likelihood for measurement $i$ in species $j$:
$$\text{number.seeds}_{ij} = a + b \; \text{biomass}_{ij} + \epsilon_{ij}$$
with $\epsilon_{ij} \sim \text{Normal}(0,\sigma^2)$

Alternatively:
$$\text{number.seeds}_{ij} \sim \text{Normal}(a + b \; \text{biomass}_{ij},\sigma^2)$$

Priors:
$$a,b \sim \text{Normal}(0,1000)$$
$$\sigma \sim \text{Uniform}(0,100)$$

# Bayesian linear regression - in Jags

## Read in and manipulate data

```{r}
# read in data
VMG <- read.table("dat/VMG.csv", header=TRUE, dec= ".", sep =";")
# nb of seeds (log)
y=log(VMG$NGrTotest)
# biomass
x=VMG$Vm
# species name
Sp=VMG$Sp
# species label
species = as.numeric(Sp)
# species name
nbspecies = length(levels(Sp))
# total nb of measurements 
n = length(y)
```

## Implement the model in Jags

```{r message=FALSE, warning=FALSE}
model <- 
paste("
model{
for(i in 1:n){
	y[i] ~ dnorm(mu[i],tau.y)
	mu[i] <- a+b*x[i]
	}
tau.y<-1/(sigma.y*sigma.y)
sigma.y~dunif(0,100)
a~dnorm(0,0.001)
b~dnorm(0,0.001)
}
")
writeLines(model,"code/pooling.bug")
```

## Prepare ingredients for running Jags

```{r}
# data
allom.data <- list(y=y,n=n,x=x)

# initial values
init1<-list(a=rnorm(1), b=rnorm(1),sigma.y=runif(1))
init2<-list(a=rnorm(1), b=rnorm(1),sigma.y=runif(1))
inits<-list(init1,init2)

# parameters to be estimated
allom.parameters <- c("a", "b", "sigma.y")
```

## Run Jags!

```{r}
allom.1 <- R2jags::jags(allom.data,inits,allom.parameters,
                        n.iter = 2500,model.file="code/pooling.bug", 
                        n.chains = 2, n.burn = 1000)
```

## Display results

```{r}
allom.1
```

## Output

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(lattice)
xyplot(y ~ x | Sp,
       xlab = "Biomass", ylab = "Nb seeds",main="complete pooling (no species effect)",
       panel = function(x, y) {
           panel.xyplot(x, y)
           panel.abline(a=c(4.63739,0.19660),col='red',lwd=3)
       })
```

# Models with mixed effects

## Varying-intercept model

```{r, fig.align='center',echo=FALSE}
knitr::include_graphics('img/varyingint.png')    
```

## Varying-intercept model

Likelihood for measurement $i$ in species $j$:
$$\text{number.seeds}_{ij} = a_j + b \; \text{biomass}_{ij} + \epsilon_{ij}$$
with $a_{j} \sim \text{Normal}(\mu_a,\sigma_a^2)$ species random effect that captures inter-species variability

and $\epsilon_{ij} \sim \text{Normal}(0,\sigma^2)$ residual variance

Priors:
$$\mu_a,b \sim \text{Normal}(0,1000)$$
$$\sigma, \sigma_a \sim \text{Uniform}(0,100)$$

## Varying-intercept model in Jags

```{r message=FALSE, warning=FALSE}
model <- paste("
model {
  for (k in 1:n){
    y[k] ~ dnorm (y.hat[k], tau.y)
    y.hat[k] <- a[species[k]] + b *x[k]}
  tau.y <- pow(sigma.y, -2)
  sigma.y ~ dunif (0, 100)
  for (j in 1:nbspecies){ a[j] ~ dnorm (mu.a, tau.a)}
  mu.a ~ dnorm (0, .001)
  tau.a <- pow(sigma.a, -2)
  sigma.a ~ dunif (0, 100)
  b ~ dnorm (0, .001)    }")
writeLines(model,"code/varint.bug")
```

```{r message=FALSE, warning=FALSE, include=FALSE}
allom.data <- list (n=n, nbspecies= nbspecies,x=x,y=y,species=species)

# on specifie le modele 
model <- 
paste("
# varying-intercept intercept allometry model 
# with biomass as a species predictor 

model {
  for (k in 1:n){
    y[k] ~ dnorm (y.hat[k], tau.y)
    y.hat[k] <- a[species[k]] + b *x[k]
  }

  tau.y <- pow(sigma.y, -2)
  sigma.y ~ dunif (0, 100)

  for (j in 1:nbspecies){
    a[j] ~ dnorm (mu.a, tau.a)
  }
  
  mu.a ~ dnorm (0, .001)
  tau.a <- pow(sigma.a, -2)
  sigma.a ~ dunif (0, 100)

  b ~ dnorm (0, .001)

}
")
writeLines(model,"code/varint.bug")

init1 <- list(a=rnorm(nbspecies), b=rnorm(1), mu.a=rnorm(1),sigma.y=runif(1), sigma.a=runif(1))
init2 <- list(a=rnorm(nbspecies), b=rnorm(1), mu.a=rnorm(1),sigma.y=runif(1), sigma.a=runif(1))
inits<-list(init1,init2)
allom.parameters <- c ("a", "b", "mu.a","sigma.y", "sigma.a")
# run JAGS
allom.2 <- R2jags::jags(allom.data,inits,allom.parameters, n.iter = 2500,model.file="code/varint.bug", n.chains = 2, n.burn = 1000)
allom.2

## graph (correction BUG 2015)
acoef.sp = allom.2$BUGSoutput$summary[1:33,1]
bcoef = allom.2$BUGSoutput$summary[34,1]

# varying-intercept predicted values
yfit = rep(0,length=n)
for (k in 1:n){yfit[k] = acoef.sp[species[k]] + bcoef * x[k]}

# pooling model (no species effect) predicted values
ylinear = rep(0,length=n)
for (k in 1:n){ylinear[k] = 4.63739 + 0.19660 * x[k]}

## define function to fit observed and predicted values in species-specific panels 
panelfun2 <- 
  function(x, y, subscripts, ...){ 
           llines(x, lmhat[subscripts], type="p") # observed data
           llines(x, hat[subscripts], type="l", lty=1,col='green',lwd=3) # varying-intercept fit
           llines(x, hat2[subscripts], type="l", lty=1,col='red',lwd=3) # pooling model (no species effect) fit
} 

# assign observed and predicted values
lmhat <- y # observed data
hat <- yfit # varying-intercept fit
hat2 <- ylinear # pooling model (no species effect) fit
```

## Compare \textcolor{red}{complete pooling} vs \textcolor{green}{varying-intercept}

```{r echo=FALSE, message=FALSE, warning=FALSE}
# build a multipanel plot 
xyplot(y ~ x | Sp, panel=panelfun2,  
       xlab="biomass", 
       ylab="nb graines",      
       key = list(text = list(c("varying-intercept", "pooling")),
       lines = list(lwd = 3, col = c("green", "red"),
       type = c("l", "l"))))
```

## Varying intercept, varying slope model

```{r echo=FALSE, fig.align='center', message=FALSE, warning=FALSE}
knitr::include_graphics('img/varyingintslo.png')    
```

```{r message=FALSE, warning=FALSE, include=FALSE}
allom.data <- list (n=n,species=species,x=x,y=y,nbspecies=nbspecies)

# on specifie le modele 
model <- 
paste("
# varying-intercept, varying-slope allometry model 
# with Vm as a species predictor 

model {
  for (k in 1:n){
    y[k] ~ dnorm (y.hat[k], tau.y)
    y.hat[k] <- a[species[k]] + b[species[k]]*x[k]
  }

  tau.y <- pow(sigma.y, -2)
  sigma.y ~ dunif (0, 100)

  for (j in 1:nbspecies){
    a[j] ~ dnorm (mu.a, tau.a)
    b[j] ~ dnorm (mu.b, tau.b)
  }
  
  mu.a ~ dnorm (0, .0001)
  tau.a <- pow(sigma.a, -2)
  sigma.a ~ dunif (0, 100)

  mu.b ~ dnorm (0, .0001)
  tau.b <- pow(sigma.b, -2)
  sigma.b ~ dunif (0, 100)

}
")
writeLines(model,"code/varintvarslope.bug")

init1 <- list (a=rnorm(nbspecies), b=rnorm(nbspecies), mu.a=rnorm(1),mu.b=rnorm(1),
sigma.y=runif(1), sigma.a=runif(1), sigma.b=runif(1))
init2 <- list (a=rnorm(nbspecies), b=rnorm(nbspecies), mu.a=rnorm(1),mu.b=rnorm(1),
sigma.y=runif(1), sigma.a=runif(1), sigma.b=runif(1))
inits<-list(init1,init2)
allom.parameters <- c ("a", "b", "mu.a","mu.b", "sigma.y", "sigma.a", "sigma.b")

allom.3 <- R2jags::jags(allom.data,inits,allom.parameters, n.iter = 2500,model.file="code/varintvarslope.bug", n.chains = 2, n.burn = 1000)
allom.3

## graph (correction BUG 2015)
acoef.sp = allom.3$BUGSoutput$summary[1:33,1]
bcoef.sp = allom.3$BUGSoutput$summary[34:66,1]

yfit2 = rep(0,length=n)
for (k in 1:n){yfit2[k] = acoef.sp[species[k]] + bcoef.sp[species[k]] * x[k]}

# pooling model (no species effect) predicted values
ylinear = rep(0,length=n)
for (k in 1:n){ylinear[k] = 4.63739 + 0.19660 * x[k]}


## define function to fit observed and predicted values in species-specific panels 
panelfun2 <- 
  function(x, y, subscripts, ...){ 
           llines(x, lmhat[subscripts], type="p",col='black') # observed data
           llines(x, hat[subscripts], type="l", lty=1,col='green',lwd=3) # varying-intercept fit
           llines(x, hat2[subscripts], type="l", lty=1,col='red',lwd=3) # pooling model (no species effect) fit
           llines(x, hat3[subscripts], type="l", lty=1,col='blue',lwd=3) # varying-intercept varying-slope fit
} 

# assign observed and predicted values
lmhat <- y # observed data
hat <- yfit # varying-intercept fit
hat2 <- ylinear # pooling model (no species effect) fit
hat3 <- yfit2 # varying-intercept varying-slope fit
```

## Compare \textcolor{red}{complete pooling} vs \textcolor{green}{varying intercept} vs \textcolor{blue}{varying intercept and slope}

```{r echo=FALSE, message=FALSE, warning=FALSE}
xyplot(y ~ x | Sp, panel=panelfun2,  
       xlab="biomass", 
       ylab="nb graines",      
       key = list(text = list(c("varying-intercept/slope", "varying-intercept","pooling")),
       lines = list(lwd = 3, col = c("blue","green","red"),
       type = c("l", "l", "l"))))
```



## Take-home message: shall I go for frequentist or Bayes?

* Pros
     + allows formal use of prior information
     + error propagation made easy
     + with same MCMC algorithms, complex (hierarchical) models can be implemented

* Cons
     + computational burden can be high
     + model selection is still difficult to perform
     + checking convergence is painful
     + is Jags too flexible?

* So what?
     + make an informed and pragmatic choice
     + are you after complexity, speed, uncertainties, etc?
     + talk to colleagues
     
     
# What about a Bayesian approach?


